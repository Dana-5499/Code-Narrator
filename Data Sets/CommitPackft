#Loading the commitpackft dataset
!pip install -q "datasets<4.0"
from datasets import load_dataset, concatenate_datasets

# list of languages to extract
LANGS = ["python", "java", "javascript", "go", "ruby", "php"]

parts = []
for lang in LANGS:
    print(f"Loading {lang}...")
    ds_lang = load_dataset("bigcode/commitpackft", lang, split="train")
    # Add an explicit 'language' column (some configs may miss it)
    ds_lang = ds_lang.map(lambda e: {"language": lang})
    parts.append(ds_lang)

# Combine into one dataset
ds = concatenate_datasets(parts)

print("\nFinal dataset")
print("Total rows:", len(ds))
print("Fields:", ds.features)
print("Sample:", ds[0])

#Splitting dataset to train, validation and test
from datasets import ClassLabel
from datasets import DatasetDict

# Make "language" a ClassLabel
langs = sorted(set(ds["language"]))
classlabel = ClassLabel(num_classes=len(langs), names=langs)
ds = ds.cast_column("language", classlabel)

# Aplitting to 70-10-20 ratio of train-validation-test
split_1 = ds.train_test_split(test_size=0.30, seed=42, stratify_by_column="language")
split_2 = split_1["test"].train_test_split(test_size=2/3, seed=42, stratify_by_column="language")

splits = DatasetDict({
    "train": split_1["train"],
    "validation": split_2["train"],
    "test": split_2["test"]
})

print("Train size:", len(splits["train"]))
print("Validation size:", len(splits["validation"]))
print("Test size:", len(splits["test"]))

#Counting number of samples from each language in each split
from collections import Counter
from datasets import ClassLabel

def print_language_distribution(ds, split_name="dataset"):
    feat = ds.features["language"]
    if isinstance(feat, ClassLabel):
        # decode integers to actual language names
        langs = [feat.int2str(i) for i in ds["language"]]
    else:
        langs = ds["language"]

    counts = Counter(langs)
    print(f"\nLanguage distribution in the {split_name} split:")
    for lang, count in sorted(counts.items(), key=lambda x: x[0].lower()):
        print(f"{lang:12s} {count}")

# For each split
print_language_distribution(splits["train"], "train")
print_language_distribution(splits["validation"], "validation")
print_language_distribution(splits["test"], "test")

#Printing three random samples and their fields from the training set
feat = splits["train"].features["language"]

def print_random_samples(ds, n=3, seed=42):
    sample_ds = ds.shuffle(seed=seed).select(range(min(n, len(ds))))
    for i, ex in enumerate(sample_ds, 1):
        lang_str = feat.int2str(ex["language"]) if isinstance(ex["language"], int) else ex["language"]
        print(f"\n{'='*40} SAMPLE {i} {'='*40}")
        print("Language:", lang_str)
        print("\n--- Subject ---")
        print(ex.get("subject", ""))
        print("\n--- Message ---")
        print(ex.get("message", ""))
        print("\n--- Old contents ---")
        print((ex.get("old_contents") or "")[:800])
        print("\n--- New contents ---")
        print((ex.get("new_contents") or "")[:800])
        print("\n" + "="*90)

# printing 3 random samples
print_random_samples(splits["train"], n=3)

#Keeping only the old contents, new contents, language, message and subject fields
def preprocess_commit(example):
    return {
        "instruction": "Explain, in plain English, what functionality changed between these two commits.",
        "old_code":   (example.get("old_contents") or "").strip(),
        "new_code":   (example.get("new_contents") or "").strip(),
        "language":   example.get("language", example.get("lang", "code")),
        "message":    (example.get("message") or "").strip(),
        "subject":    (example.get("subject") or "").strip(),
    }

# apply to all splits
for split_name in ["train", "validation", "test"]:
    splits[split_name] = splits[split_name].map(
        preprocess_commit,
        remove_columns=splits[split_name].column_names
    )

print(splits)
print("Train sample:", splits["train"][0])

#Filtering the dataset:
#combining subject+message fields into one + cleaning up white space
#removes too short commits or commits which are less human-readable
import re

# Extract a cleaned target (combine subject + message)
def extract_commit_message(subject, message):
    # prefer subject, append message if it adds new info
    subj = (subject or "").strip()
    msg  = (message or "").strip()
    if msg and msg.lower() != subj.lower():
        combined = subj + "\n" + msg if subj else msg
    else:
        combined = subj or msg

    # normalize whitespace
    combined = re.sub(r"\s+", " ", combined).strip()
    return combined if combined else None


def is_good_commit(msg):
    if msg is None:
        return False
    # too short
    if len(msg) < 20:
        return False
    # must start with uppercase
    if not msg[0].isupper():
        return False
    # must end with punctuation
    if not msg.endswith((".", "?", "!")):
        return False
    # discard boilerplate
    bad_starts = (
        "Merge branch", "Merge pull request",
        "WIP", "Temp", "Test", "Update README",
        "Bump version", "Release"
    )
    if msg.startswith(bad_starts):
        return False
    return True

def preprocess_and_filter(ds):
    # combine + clean message
    ds = ds.map(lambda ex: {
        **ex,
        "target": extract_commit_message(ex.get("subject"), ex.get("message"))
    })
    # filter out bad/None
    ds = ds.filter(lambda ex: is_good_commit(ex["target"]))
    return ds

splits["train"] = preprocess_and_filter(splits["train"])
splits["validation"] = preprocess_and_filter(splits["validation"])
splits["test"] = preprocess_and_filter(splits["test"])

print("Train size:", len(splits["train"]))
print("Validation size:", len(splits["validation"]))
print("Test size:", len(splits["test"]))


#Saving the entire dataset to drive
from google.colab import drive
import os, shutil, glob, json

# Unmount if partially mounted
try:
    drive.flush_and_unmount()
except Exception:
    pass

# Remove cached credentials so you get the account chooser
for p in glob.glob("/root/.config/drive/*"):
    try: shutil.rmtree(p)
    except:
        try: os.remove(p)
        except: pass

# If the mountpoint has leftover files, clear it so Colab can mount there
if os.path.exists("/content/drive") and os.listdir("/content/drive"):
    # This only removes the *mountpoint contents in the runtime*, not your real Drive
    for name in os.listdir("/content/drive"):
        path = os.path.join("/content/drive", name)
        try:
            shutil.rmtree(path)
        except:
            try: os.remove(path)
            except: pass

# Mount (approve and select the correct Google account)
drive.mount('/content/drive', force_remount=True)


# Show the mounted account email
import glob, json
paths = glob.glob("/root/.config/drive/credentials.json")
print("Cred files:", paths)
if paths:
    with open(paths[0]) as f:
        creds = json.load(f)
    print("Mounted Google account email:", creds.get("client_user_email", "<unknown>"))
else:
    print("No credentials.json found (not mounted).")

# List top-level of MyDrive
!ls -lah /content/drive/MyDrive | sed -n '1,200p'

from datasets import DatasetDict
import os

# wrap the (already filtered) splits into a DatasetDict
ds = DatasetDict({
    "train":      splits["train"],
    "validation": splits["validation"],
    "test":       splits["test"]
})

# save to Drive
save_dir = "/content/drive/MyDrive/commitpackft_processed"
os.makedirs(save_dir, exist_ok=True)
ds.save_to_disk(save_dir)
print("Saved processed dataset to", save_dir)

# --- check sizes ---
print("Train size:", len(ds["train"]))
print("Validation size:", len(ds["validation"]))
print("Test size:", len(ds["test"]))

from google.colab import drive

# --- check which Drive account is mounted ---
import os

# list the top-level of MyDrive
print("Top of MyDrive:")
!ls -lah /content/drive/MyDrive | head -20

# write a marker file
with open("/content/drive/MyDrive/WHOAMI.txt", "w") as f:
    f.write("Hello from this Colab runtime.\n")

print("\nMarker file created at: /content/drive/MyDrive/WHOAMI.txt")

import json, os, glob

cred_paths = glob.glob("/root/.config/drive/credentials.json")
print("Cred files:", cred_paths)
if cred_paths:
    with open(cred_paths[0], "r") as f:
        data = json.load(f)
    # try a few common keys
    email = data.get("client_user_email") or data.get("email") or data.get("user_email")
    print("Mounted Google account email:", email)
else:
    print("No credentials.json found (Drive may not be mounted).")


#Splitting each of the preprocessed splits per processing language
from collections import defaultdict
from datasets import DatasetDict, ClassLabel

def split_by_language_commits(splits):
    train_ds, val_ds, test_ds = splits["train"], splits["validation"], splits["test"]
    lang_splits = defaultdict(dict)

    # check if language is a ClassLabel
    feat = train_ds.features["language"]
    if isinstance(feat, ClassLabel):
        # decode ints to strings
        train_langs = [feat.int2str(x) for x in train_ds["language"]]
        val_langs   = [feat.int2str(x) for x in val_ds["language"]]
        test_langs  = [feat.int2str(x) for x in test_ds["language"]]
    else:
        train_langs, val_langs, test_langs = train_ds["language"], val_ds["language"], test_ds["language"]

    all_langs = set(train_langs) | set(val_langs) | set(test_langs)

    for lang in all_langs:
        lang_splits[lang]["train"] = train_ds.filter(lambda e: (feat.int2str(e["language"]) if isinstance(feat, ClassLabel) else e["language"]) == lang)
        lang_splits[lang]["validation"] = val_ds.filter(lambda e: (feat.int2str(e["language"]) if isinstance(feat, ClassLabel) else e["language"]) == lang)
        lang_splits[lang]["test"] = test_ds.filter(lambda e: (feat.int2str(e["language"]) if isinstance(feat, ClassLabel) else e["language"]) == lang)

    return {lang: DatasetDict(splits) for lang, splits in lang_splits.items()}

# build per-language dataset dicts
lang_datasets = split_by_language_commits(splits)

# print stats per language
for lang, dsets in lang_datasets.items():
    print(f"\n=== {lang.upper()} ===")
    for split_name, ds in dsets.items():
        print(f"{split_name}: {len(ds)} samples")

#Compacting datasets further to contain 2100 train, 300 validation, 600 test max for every language + loading to google drive
# ==================== Compact subsets for EVERY language (CommitPackFT) ====================
import os, re, hashlib, random
from datasets import DatasetDict

TARGETS = {"train": 2_100, "validation": 300, "test": 600}
random.seed(42)

# -- helpers: support either renamed fields (old_code/new_code) or raw (old_contents/new_contents)
def _get_before_after(ex):
    before = ex.get("old_code") or ex.get("old_contents") or ""
    after  = ex.get("new_code") or ex.get("new_contents") or ""
    return before, after

def good_change_length(ex, min_lines=3, max_lines=350):
    before, after = _get_before_after(ex)
    n = (before.count("\n") + 1) + (after.count("\n") + 1)  # combined lines
    return (min_lines <= n <= max_lines)

def add_change_hash(ex):
    before, after = _get_before_after(ex)
    # Use a stable hash of the before/after pair
    ex["__change_hash"] = hashlib.md5(
        (before.strip() + "\n<SEP>\n" + after.strip()).encode("utf-8","ignore")
    ).hexdigest()
    return ex

def unique_by_change_hash():
    seen = set()
    def _f(e):
        h = e["__change_hash"]
        if h in seen:
            return False
        seen.add(h)
        return True
    return _f

def reduce_and_sample(ds, target, seed=42):
    if len(ds) == 0:
        return ds
    # 1) simple length filter on the change size
    ds = ds.filter(good_change_length, desc="len filter")
    # 2) deduplicate identical (before, after) pairs
    ds = ds.map(add_change_hash, desc="hashing")
    ds = ds.filter(unique_by_change_hash(), desc="dedup")
    # 3) random sample up to cap
    keep = min(target, len(ds))
    ds_small = ds.shuffle(seed=seed).select(range(keep))
    # 4) cleanup temp column
    if "__change_hash" in ds_small.column_names:
        ds_small = ds_small.remove_columns(["__change_hash"])
    return ds_small

per_lang_compact = {}
for lang, splits in lang_datasets.items():  # <- reuse your per-language splits dict
    compact_train = reduce_and_sample(splits["train"],      TARGETS["train"])
    compact_val   = reduce_and_sample(splits["validation"], TARGETS["validation"])
    compact_test  = reduce_and_sample(splits["test"],       TARGETS["test"])
    per_lang_compact[lang] = DatasetDict({
        "train": compact_train,
        "validation": compact_val,
        "test": compact_test
    })

# print counts (handle ClassLabel keys or strings)
def _to_str(x):
    try:
        return str(x).upper()
    except Exception:
        return str(x)

for lang, dsets in per_lang_compact.items():
    print(f"\n=== {_to_str(lang)} (compact) ===")
    for split_name, ds_ in dsets.items():
        print(f"{split_name}: {len(ds_)} samples")

# optional: save
base_dir = "/content/drive/MyDrive/commitpackft_compact_by_lang"
os.makedirs(base_dir, exist_ok=True)
for lang, dsets in per_lang_compact.items():
    safe = re.sub(r"[^a-zA-Z0-9_+-]+", "_", str(lang).lower())
    outdir = os.path.join(base_dir, safe)
    dsets.save_to_disk(outdir)
    print(f"Saved {lang} compact dataset to {outdir}")
# ========================================================================

